meta data:
{   'ann_output_processor': BatchProcessor([('sgmd', slice(0, 201, None), BatchProcessor([]))]),
    'dt': 900,
    'input_processor': BatchProcessor([('nmlz', slice(0, 1, None), BatchProcessor([]), tensor([0.]), tensor([32400000.])), ('1', slice(1, 3, None), BatchProcessor([]))]),
    'model': BESS(state=[27071274.        0.        1.], capacity=32400000, charging_efficiency=0.83, discharging_efficiency=1, relative_loss=0.0075, correct_infeasible=True),
    'output_action_feasibility': True,
    'output_interaction': False,
    'output_new_state': False,
    'output_processor': None,
    'sampling_parameters': {   'infeasible_chance': 0.6666666666666666,
                               'soc_distribution': (   [   (0, 0.25),
                                                           (0.25, 0.75),
                                                           (0.75, 1)],
                                                       [0.375, 0.25, 0.375])}}
---
parameters:
{   'batch_count': 1000.0,
    'batch_norms': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),
    'batch_size': 3072,
    'betas': array([0.5, 2. , 1. , 0.5, 1. , 2. , 1. , 2. , 2. , 1. ]),
    'dropout': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),
    'early_stopping_callback': EarlyStoppingCallback(stopping_scores={}, improvement_window=100),
    'epoch_count': 1000,
    'hidden_layer_count': 5,
    'input_width': 3,
    'learning_rate': 0.005,
    'loss': MixedLoss([(BCEWithLogitsLoss(), 201)],tensor([1.], device='cuda:0'),cuda),
    'lr_scheduler': array([<class 'torch.optim.lr_scheduler.StepLR'>, {'step_size': 1, 'gamma': 0.99}], dtype=object),
    'max_grad_norm': 1000000.0,
    'output_activation': None,
    'output_width': 201,
    'regularization': L1RegularizationLoss(device=cuda, scale=2.000000E-09),
    'skips': array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
    'width': 512,
    'width_interpolation_steps_input': 0,
    'width_interpolation_steps_output': 0}
---
neural network:
Sequential(
  (0_linear(3,512)): Linear(in_features=3, out_features=512, bias=True)
  (0_swish(2'00)): Swish(
    (sigmoid): Sigmoid()
  )
  (1_skip_to_2(512)): SkipConnection()
  (1_linear(512,512)): Linear(in_features=512, out_features=512, bias=True)
  (1_swish(1'00)): Swish(
    (sigmoid): Sigmoid()
  )
  (2_skip_from_1(512)): SkipConnection()
  (2_linear(1024,512)): Linear(in_features=1024, out_features=512, bias=True)
  (2_swish(2'00)): Swish(
    (sigmoid): Sigmoid()
  )
  (3_skip_to_4(512)): SkipConnection()
  (3_linear(512,512)): Linear(in_features=512, out_features=512, bias=True)
  (3_swish(2'00)): Swish(
    (sigmoid): Sigmoid()
  )
  (4_skip_from_3(512)): SkipConnection()
  (4_linear(1024,512)): Linear(in_features=1024, out_features=512, bias=True)
  (4_swish(1'00)): Swish(
    (sigmoid): Sigmoid()
  )
  (5_linear(512,201)): Linear(in_features=512, out_features=201, bias=True)
)
---
parameter count:
1680078
---
